import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from loguru import logger
from typing import Optional, Dict, Any

class BloomzService:
    """Service class for Bloomz model interactions using Hugging Face transformers"""
    
    def __init__(self, model_name: str = "bigscience/bloomz-560m"):
        """
        Initialize Bloomz service with model loading
        
        Args:
            model_name: Hugging Face model name for Bloomz
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Load model on initialization (can be changed to lazy loading)
        self._load_model()
    
    def _load_model(self):
        """Load the Bloomz model and tokenizer"""
        try:
            logger.info(f"Loading Bloomz model: {self.model_name}")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Load model with appropriate settings
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )
            
            # Create text generation pipeline
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1
            )
            
            logger.info(f"Bloomz model loaded successfully on {self.device}")
            
        except Exception as e:
            logger.error(f"Failed to load Bloomz model: {str(e)}")
            self.tokenizer = None
            self.model = None
            self.pipeline = None
    
    def generate_text(self, prompt: str, max_length: int = 100,
                     temperature: float = 0.7, top_p: float = 0.9,
                     do_sample: bool = True, num_return_sequences: int = 1) -> str:
        """
        Generate text using Bloomz model
        
        Args:
            prompt: The input prompt
            max_length: Maximum length of generated text
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
            do_sample: Whether to use sampling
            num_return_sequences: Number of sequences to return
            
        Returns:
            Generated text
        """
        if not self.pipeline:
            raise Exception("Bloomz model not loaded. Please check model initialization.")
        
        try:
            # Generate text using the pipeline
            outputs = self.pipeline(
                prompt,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                num_return_sequences=num_return_sequences,
                pad_token_id=self.tokenizer.eos_token_id,
                return_full_text=False
            )
            
            # Extract generated text
            if outputs and len(outputs) > 0:
                generated_text = outputs[0]['generated_text']
                return generated_text.strip()
            else:
                raise Exception("No text generated by Bloomz model")
                
        except Exception as e:
            logger.error(f"Bloomz text generation error: {str(e)}")
            raise Exception(f"Bloomz text generation error: {str(e)}")
    
    def chat_completion(self, prompt: str, max_length: int = 100,
                       temperature: float = 0.7) -> str:
        """
        Generate chat-style completion using Bloomz model
        
        Args:
            prompt: The input prompt
            max_length: Maximum length of generated text
            temperature: Sampling temperature
            
        Returns:
            Generated response text
        """
        # Format prompt for chat-style interaction
        formatted_prompt = f"Human: {prompt}\nAssistant:"
        
        return self.generate_text(
            formatted_prompt,
            max_length=max_length,
            temperature=temperature
        )
    
    def batch_generate(self, prompts: list, max_length: int = 100,
                      temperature: float = 0.7) -> list:
        """
        Generate text for multiple prompts in batch
        
        Args:
            prompts: List of input prompts
            max_length: Maximum length of generated text
            temperature: Sampling temperature
            
        Returns:
            List of generated texts
        """
        if not self.pipeline:
            raise Exception("Bloomz model not loaded. Please check model initialization.")
        
        try:
            results = []
            for prompt in prompts:
                result = self.generate_text(
                    prompt,
                    max_length=max_length,
                    temperature=temperature
                )
                results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"Bloomz batch generation error: {str(e)}")
            raise Exception(f"Bloomz batch generation error: {str(e)}")
    
    def translate_text(self, text: str, target_language: str = "French",
                      max_length: int = 200) -> str:
        """
        Translate text using Bloomz model
        
        Args:
            text: Text to translate
            target_language: Target language for translation
            max_length: Maximum length of translation
            
        Returns:
            Translated text
        """
        prompt = f"Translate the following text to {target_language}: {text}\nTranslation:"
        
        return self.generate_text(
            prompt,
            max_length=max_length,
            temperature=0.3  # Lower temperature for translation
        )
    
    def summarize_text(self, text: str, max_length: int = 150) -> str:
        """
        Summarize text using Bloomz model
        
        Args:
            text: Text to summarize
            max_length: Maximum length of summary
            
        Returns:
            Summary text
        """
        prompt = f"Summarize the following text:\n{text}\nSummary:"
        
        return self.generate_text(
            prompt,
            max_length=max_length,
            temperature=0.5
        )
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the loaded model
        
        Returns:
            Dictionary with model information
        """
        if not self.model:
            return {"status": "Model not loaded"}
        
        return {
            "model_name": self.model_name,
            "device": self.device,
            "model_size": sum(p.numel() for p in self.model.parameters()),
            "vocab_size": len(self.tokenizer) if self.tokenizer else None,
            "status": "Model loaded successfully"
        }
    
    def is_configured(self) -> bool:
        """Check if Bloomz service is properly configured"""
        return self.pipeline is not None
    
    def unload_model(self):
        """Unload the model to free memory"""
        if self.model:
            del self.model
            self.model = None
        
        if self.tokenizer:
            del self.tokenizer
            self.tokenizer = None
        
        if self.pipeline:
            del self.pipeline
            self.pipeline = None
        
        # Clear CUDA cache if using GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("Bloomz model unloaded successfully") 